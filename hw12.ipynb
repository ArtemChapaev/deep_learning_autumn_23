{
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Домашнее задание : \"Обучение с подкреплением\""
   ],
   "metadata": {
    "id": "3801d352"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "ФИО: Чапаев Артем Юрьевич"
   ],
   "metadata": {
    "id": "e0c3d037"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install setuptools\n",
    "!pip install -v cmake gymnasium 'gym[toy_text]' gym[atari]\n",
    "!pip install gym[accept-rom-license]\n",
    "!pip install atari-py\n",
    "!pip install stable-baselines3[extra] pyvirtualdisplay gym[atari] pyglet "
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T18:55:12.578132Z",
     "iopub.execute_input": "2023-12-30T18:55:12.579056Z",
     "iopub.status.idle": "2023-12-30T18:58:46.234439Z",
     "shell.execute_reply.started": "2023-12-30T18:55:12.579024Z",
     "shell.execute_reply": "2023-12-30T18:58:46.233411Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (68.1.2)\nUsing pip 23.2.1 from /opt/conda/lib/python3.10/site-packages/pip (python 3.10)\nCollecting cmake\n  Obtaining dependency information for cmake from https://files.pythonhosted.org/packages/09/30/df85689d18122becb9b6495cf6778f9ef629bdaa3ec86f49809ab5772e35/cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n  Downloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\nRequirement already satisfied: gymnasium in /opt/conda/lib/python3.10/site-packages (0.29.0)\nRequirement already satisfied: gym[toy_text] in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (1.24.3)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (2.2.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (4.5.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium) (0.0.4)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[toy_text]) (0.0.8)\nCollecting pygame==2.1.0 (from gym[toy_text])\n  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m18.3/18.3 MB\u001B[0m \u001B[31m50.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hCollecting ale-py~=0.8.0 (from gym[toy_text])\n  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/1.7 MB\u001B[0m \u001B[31m58.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[toy_text]) (5.13.0)\nDownloading cmake-3.28.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (26.3 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.3/26.3 MB\u001B[0m \u001B[31m42.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: cmake, pygame, ale-py\n  changing mode of /opt/conda/bin/cmake to 755\n  changing mode of /opt/conda/bin/cpack to 755\n  changing mode of /opt/conda/bin/ctest to 755\n  changing mode of /opt/conda/bin/ale-import-roms to 755\nSuccessfully installed ale-py-0.8.1 cmake-3.28.1 pygame-2.1.0\nRequirement already satisfied: gym[accept-rom-license] in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license]) (1.24.3)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license]) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license]) (0.0.8)\nCollecting autorom[accept-rom-license]~=0.4.2 (from gym[accept-rom-license])\n  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (4.66.1)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license])\n  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m434.7/434.7 kB\u001B[0m \u001B[31m7.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25h  Installing build dependencies ... \u001B[?25ldone\n\u001B[?25h  Getting requirements to build wheel ... \u001B[?25ldone\n\u001B[?25h  Preparing metadata (pyproject.toml) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license]) (2023.11.17)\nBuilding wheels for collected packages: AutoROM.accept-rom-license\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=afcae23c7851eeb538df511b4db698bd5d57e59face9659ade0c3b7b8791252b\n  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\nSuccessfully built AutoROM.accept-rom-license\nInstalling collected packages: AutoROM.accept-rom-license, autorom\nSuccessfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.4.2\nCollecting atari-py\n  Downloading atari-py-0.2.9.tar.gz (540 kB)\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m540.6/540.6 kB\u001B[0m \u001B[31m7.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25ldone\n\u001B[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from atari-py) (1.24.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from atari-py) (1.16.0)\nBuilding wheels for collected packages: atari-py\n  Building wheel for atari-py (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for atari-py: filename=atari_py-0.2.9-cp310-cp310-linux_x86_64.whl size=2857331 sha256=5c92e94fc4cfe9170fb6668fce38cebeb73df4031e991c5ac7be9712229caf43\n  Stored in directory: /root/.cache/pip/wheels/75/6f/04/1f3bf5255580101e16ff487564354dddcdd23ec3b43b775b7a\nSuccessfully built atari-py\nInstalling collected packages: atari-py\nSuccessfully installed atari-py-0.2.9\nRequirement already satisfied: stable-baselines3[extra] in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting pyvirtualdisplay\n  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\nRequirement already satisfied: gym[atari] in /opt/conda/lib/python3.10/site-packages (0.26.2)\nCollecting pyglet\n  Obtaining dependency information for pyglet from https://files.pythonhosted.org/packages/e9/33/cbff7525a357c950e76717ea9741127a662a7ed49a92874897b8a4036db9/pyglet-2.0.10-py3-none-any.whl.metadata\n  Downloading pyglet-2.0.10-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: gymnasium<0.30,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (0.29.0)\nRequirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (1.24.3)\nRequirement already satisfied: torch>=1.13 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.0.0+cpu)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.2.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.0.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.7.4)\nRequirement already satisfied: opencv-python in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.8.1.78)\nRequirement already satisfied: pygame in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.1.0)\nRequirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.13.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (5.9.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.66.1)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (13.5.2)\nCollecting shimmy[atari]~=1.1.0 (from stable-baselines3[extra])\n  Obtaining dependency information for shimmy[atari]~=1.1.0 from https://files.pythonhosted.org/packages/d5/fb/083e36bbcf325f6304bbeb2278b102c4ac8e87eb1ca771780f64decbb2f1/Shimmy-1.1.0-py3-none-any.whl.metadata\n  Downloading Shimmy-1.1.0-py3-none-any.whl.metadata (3.3 kB)\nRequirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from stable-baselines3[extra]) (10.1.0)\nCollecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra])\n  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[atari]) (0.0.8)\nRequirement already satisfied: ale-py~=0.8.0 in /opt/conda/lib/python3.10/site-packages (from gym[atari]) (0.8.1)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]) (5.13.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]) (4.5.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\nRequirement already satisfied: AutoROM.accept-rom-license in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (0.6.1)\nRequirement already satisfied: farama-notifications>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (0.0.4)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.57.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.22.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.4.4)\nRequirement already satisfied: protobuf>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (68.1.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.1)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.10/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.41.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.1.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (4.42.1)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2023.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->stable-baselines3[extra]) (2.16.1)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\nDownloading pyglet-2.0.10-py3-none-any.whl (858 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m858.3/858.3 kB\u001B[0m \u001B[31m11.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m0:01\u001B[0m\n\u001B[?25hDownloading Shimmy-1.1.0-py3-none-any.whl (37 kB)\nInstalling collected packages: pyvirtualdisplay, pyglet, shimmy, autorom\n  Attempting uninstall: shimmy\n    Found existing installation: Shimmy 1.3.0\n    Uninstalling Shimmy-1.3.0:\n      Successfully uninstalled Shimmy-1.3.0\n  Attempting uninstall: autorom\n    Found existing installation: AutoROM 0.4.2\n    Uninstalling AutoROM-0.4.2:\n      Successfully uninstalled AutoROM-0.4.2\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkaggle-environments 1.14.3 requires shimmy>=1.2.1, but you have shimmy 1.1.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mSuccessfully installed autorom-0.6.1 pyglet-2.0.10 pyvirtualdisplay-3.0 shimmy-1.1.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install imageio[ffmpeg]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T19:31:14.772257Z",
     "iopub.execute_input": "2023-12-30T19:31:14.772700Z",
     "iopub.status.idle": "2023-12-30T19:31:29.975306Z",
     "shell.execute_reply.started": "2023-12-30T19:31:14.772667Z",
     "shell.execute_reply": "2023-12-30T19:31:29.974313Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: imageio[ffmpeg] in /opt/conda/lib/python3.10/site-packages (2.31.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from imageio[ffmpeg]) (1.24.3)\nRequirement already satisfied: pillow>=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio[ffmpeg]) (10.1.0)\nCollecting imageio-ffmpeg (from imageio[ffmpeg])\n  Obtaining dependency information for imageio-ffmpeg from https://files.pythonhosted.org/packages/1a/98/3df1d8dd8f2c121b6c588b1e0d604f36592d56df9c41fb155ed546c6a5ed/imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl.metadata\n  Downloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl.metadata (1.7 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from imageio[ffmpeg]) (5.9.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from imageio-ffmpeg->imageio[ffmpeg]) (68.1.2)\nDownloading imageio_ffmpeg-0.4.9-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m26.9/26.9 MB\u001B[0m \u001B[31m42.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hInstalling collected packages: imageio-ffmpeg\nSuccessfully installed imageio-ffmpeg-0.4.9\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import gymnasium as gym"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T18:58:46.237591Z",
     "iopub.execute_input": "2023-12-30T18:58:46.237923Z",
     "iopub.status.idle": "2023-12-30T18:58:47.462828Z",
     "shell.execute_reply.started": "2023-12-30T18:58:46.237892Z",
     "shell.execute_reply": "2023-12-30T18:58:47.461970Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание 1\n",
    "\n",
    "Обучите алгоритм Q-learning для сред FrozenLake-v1 и Blackjack-v1, в частности подберите оптимальную alpha. (2 балла)"
   ],
   "metadata": {
    "id": "9528f5be"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros((*state_space, action_space))\n",
    "    return Qtable"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # Exploitation: take the action with the highest state, action value\n",
    "    action = np.argmax(Qtable[state][:])\n",
    "\n",
    "    return action\n",
    "\n",
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # Randomly generate a number between 0 and 1\n",
    "    random_num = random.random()\n",
    "    # if random_num > greater than epsilon --> exploitation\n",
    "    if random_num > epsilon:\n",
    "        # Take the action with the highest value given a state\n",
    "        # np.argmax can be useful here\n",
    "        action = greedy_policy(Qtable, state)\n",
    "    # else --> exploration\n",
    "    else:\n",
    "        action = env.action_space.sample() # Take a random action\n",
    "\n",
    "    return action"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train(n_training_episodes, learning_rate, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in tqdm(range(n_training_episodes)):\n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate*episode)\n",
    "        # Reset the environment\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        step = 0\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        # repeat\n",
    "        for step in range(max_steps):\n",
    "            # Choose the action At using epsilon greedy policy\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # Take action At and observe Rt+1 and St+1\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "            # If terminated or truncated finish the episode\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            # Our next state is the new state\n",
    "            state = new_state\n",
    "    return Qtable"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
    "    :param env: The evaluation environment\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param Q: The Q-table\n",
    "    :param seed: The evaluation seed array (for FrozenLake-v1)\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        state, info = env.reset()\n",
    "\n",
    "        truncated = False\n",
    "        terminated = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take the action (index) that have the maximum expected future reward given that state\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, terminated, truncated, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    return np.mean(episode_rewards), np.std(episode_rewards)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FrozenLake"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "env.reset()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "action_to_i = {\n",
    "    'left': 0,\n",
    "    'down': 1,\n",
    "    'right': 2,\n",
    "    'up': 3\n",
    "}"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "state_space = env.observation_space.n\n",
    "print(\"There are \", f\"{state_space}\", \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Qtable_frozenlake = initialize_q_table([state_space], action_space)\n",
    "print(\"Q-table shape: \", Qtable_frozenlake.shape)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 1_500_000   # Total training episodes\n",
    "learning_rate = 0.2          # Learning rate\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 100        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "eval_seed = []               # The evaluation seed of the environment\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability\n",
    "decay_rate = 0.0005            # Exponential decay rate for exploration prob"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "С помощью перебора по сетке были определены наилучшие гиперпараметры"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "Qtable_frozenlake = initialize_q_table([state_space], action_space)\n",
    "Qtable_frozenlake = train(n_training_episodes, learning_rate, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)\n",
    "evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Blackjack"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "env = gym.make(\"Blackjack-v1\")\n",
    "\n",
    "env.reset()"
   ],
   "metadata": {
    "is_executing": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "action_to_i = {\n",
    "    'stick':0,\n",
    "    'hit':1\n",
    "}"
   ],
   "metadata": {
    "is_executing": true,
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "state_space = [space.n for space in env.observation_space]\n",
    "print(\"There are \", f\"{state_space[0]}x{state_space[1]}x{state_space[2]}\", \" possible states\")\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print(\"There are \", action_space, \" possible actions\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "Qtable_blackjack = initialize_q_table(state_space, action_space)\n",
    "print(\"Q-table shape: \", Qtable_blackjack.shape)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Training parameters\n",
    "n_training_episodes = 500_000   # Total training episodes\n",
    "\n",
    "# Evaluation parameters\n",
    "n_eval_episodes = 500        # Total number of test episodes\n",
    "\n",
    "# Environment parameters\n",
    "env_id = \"Blackjack-v1\"     # Name of the environment\n",
    "max_steps = 99               # Max steps per episode\n",
    "gamma = 0.95                 # Discounting rate\n",
    "\n",
    "# Exploration parameters\n",
    "max_epsilon = 1.0             # Exploration probability at start\n",
    "min_epsilon = 0.05            # Minimum exploration probability "
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "С помощью перебора по сетке были определены наилучшие гиперпараметры (какие именно были результаты можно посмотреть в первом сохранении ноутбука)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "lr = 0.3\n",
    "dr = 0.05\n",
    "\n",
    "Qtable_blackjack = initialize_q_table(state_space, action_space)\n",
    "Qtable_blackjack = train(n_training_episodes, lr, min_epsilon, max_epsilon, dr, env, max_steps, Qtable_blackjack)\n",
    "evaluate_agent(env, max_steps, n_eval_episodes, Qtable_blackjack)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lr = 0.5\n",
    "dr = 0.25\n",
    "\n",
    "Qtable_blackjack = initialize_q_table(state_space, action_space)\n",
    "Qtable_blackjack = train(n_training_episodes, lr, min_epsilon, max_epsilon, dr, env, max_steps, Qtable_blackjack)\n",
    "evaluate_agent(env, max_steps, n_eval_episodes, Qtable_blackjack)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Задание 2\n",
    "\n",
    "Обучите алгоритм Policy Gradients (или Actor Critic) для среды https://www.gymlibrary.dev/environments/atari/breakout/ . Продемонстрируйте, что для обученного агента растет время игры. (3 балла)"
   ],
   "metadata": {
    "id": "89eab25f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для этого задания использована готовую модель Proximal Policy Optimization из библиотеки stable_baseline3. Модель реализована на основе алгоритма Policy Optimization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback"
   ],
   "metadata": {
    "id": "oxUCo_3oEKyD",
    "execution": {
     "iopub.status.busy": "2023-12-30T18:58:47.464287Z",
     "iopub.execute_input": "2023-12-30T18:58:47.465356Z",
     "iopub.status.idle": "2023-12-30T18:59:04.044620Z",
     "shell.execute_reply.started": "2023-12-30T18:58:47.465311Z",
     "shell.execute_reply": "2023-12-30T18:59:04.043614Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "env = make_atari_env('Breakout-v4', n_envs=4)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "_ = env.reset()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T18:59:04.047076Z",
     "iopub.execute_input": "2023-12-30T18:59:04.047788Z",
     "iopub.status.idle": "2023-12-30T18:59:05.952747Z",
     "shell.execute_reply.started": "2023-12-30T18:59:04.047755Z",
     "shell.execute_reply": "2023-12-30T18:59:05.951732Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model = PPO('CnnPolicy', env, verbose=1)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T18:59:05.954123Z",
     "iopub.execute_input": "2023-12-30T18:59:05.954471Z",
     "iopub.status.idle": "2023-12-30T18:59:06.281995Z",
     "shell.execute_reply.started": "2023-12-30T18:59:05.954442Z",
     "shell.execute_reply": "2023-12-30T18:59:06.280923Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "Using cpu device\nWrapping the env in a VecTransposeImage.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_breakout_time(env, model):\n",
    "    episodes = 10\n",
    "    \n",
    "    steps = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = np.array([False] * 4)\n",
    "        score = 0\n",
    "\n",
    "        while not done.any():\n",
    "            action , _ = model.predict(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            steps += 1\n",
    "        print(f\"Episode: {episode} | Score: {score}\")\n",
    "    \n",
    "    print(f'\\nMean game time: {steps / episodes:0.1f} frames')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T18:59:06.283240Z",
     "iopub.execute_input": "2023-12-30T18:59:06.284922Z",
     "iopub.status.idle": "2023-12-30T18:59:06.292458Z",
     "shell.execute_reply.started": "2023-12-30T18:59:06.284887Z",
     "shell.execute_reply": "2023-12-30T18:59:06.291634Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_breakout_time(env, model)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T19:34:10.434472Z",
     "iopub.execute_input": "2023-12-30T19:34:10.434935Z",
     "iopub.status.idle": "2023-12-30T19:34:11.526697Z",
     "shell.execute_reply.started": "2023-12-30T19:34:10.434900Z",
     "shell.execute_reply": "2023-12-30T19:34:11.525461Z"
    },
    "trusted": true
   },
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "text": "Episode: 0 | Score: [0. 0. 0. 0.]\nEpisode: 1 | Score: [0. 0. 1. 0.]\nEpisode: 2 | Score: [0. 0. 0. 0.]\nEpisode: 3 | Score: [0. 0. 0. 0.]\nEpisode: 4 | Score: [0. 0. 1. 0.]\nEpisode: 5 | Score: [0. 1. 0. 0.]\nEpisode: 6 | Score: [0. 0. 0. 0.]\nEpisode: 7 | Score: [0. 0. 0. 0.]\nEpisode: 8 | Score: [0. 0. 0. 0.]\nEpisode: 9 | Score: [0. 0. 0. 0.]\n\nMean game time: 2.5 frames\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.learn(total_timesteps=100_000, progress_bar=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T18:59:07.837116Z",
     "iopub.execute_input": "2023-12-30T18:59:07.837485Z",
     "iopub.status.idle": "2023-12-30T19:28:48.957062Z",
     "shell.execute_reply.started": "2023-12-30T18:59:07.837455Z",
     "shell.execute_reply": "2023-12-30T19:28:48.955867Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "29913890bbb843728978e74f2a442aa4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "---------------------------------\n| rollout/           |          |\n|    ep_len_mean     | 276      |\n|    ep_rew_mean     | 1.44     |\n| time/              |          |\n|    fps             | 147      |\n|    iterations      | 1        |\n|    time_elapsed    | 55       |\n|    total_timesteps | 8192     |\n---------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 307         |\n|    ep_rew_mean          | 2.03        |\n| time/                   |             |\n|    fps                  | 84          |\n|    iterations           | 2           |\n|    time_elapsed         | 193         |\n|    total_timesteps      | 16384       |\n| train/                  |             |\n|    approx_kl            | 0.016362276 |\n|    clip_fraction        | 0.113       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.38       |\n|    explained_variance   | -0.00709    |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.00201    |\n|    n_updates            | 10          |\n|    policy_gradient_loss | -0.0114     |\n|    value_loss           | 0.0748      |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 374         |\n|    ep_rew_mean          | 3.35        |\n| time/                   |             |\n|    fps                  | 74          |\n|    iterations           | 3           |\n|    time_elapsed         | 331         |\n|    total_timesteps      | 24576       |\n| train/                  |             |\n|    approx_kl            | 0.025984935 |\n|    clip_fraction        | 0.243       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.33       |\n|    explained_variance   | 0.559       |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.0524     |\n|    n_updates            | 20          |\n|    policy_gradient_loss | -0.0409     |\n|    value_loss           | 0.0833      |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 382         |\n|    ep_rew_mean          | 3.57        |\n| time/                   |             |\n|    fps                  | 69          |\n|    iterations           | 4           |\n|    time_elapsed         | 470         |\n|    total_timesteps      | 32768       |\n| train/                  |             |\n|    approx_kl            | 0.025204692 |\n|    clip_fraction        | 0.279       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.28       |\n|    explained_variance   | 0.586       |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.0262     |\n|    n_updates            | 30          |\n|    policy_gradient_loss | -0.0545     |\n|    value_loss           | 0.104       |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 407         |\n|    ep_rew_mean          | 4.1         |\n| time/                   |             |\n|    fps                  | 67          |\n|    iterations           | 5           |\n|    time_elapsed         | 608         |\n|    total_timesteps      | 40960       |\n| train/                  |             |\n|    approx_kl            | 0.033271603 |\n|    clip_fraction        | 0.332       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.24       |\n|    explained_variance   | 0.621       |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.0447     |\n|    n_updates            | 40          |\n|    policy_gradient_loss | -0.0663     |\n|    value_loss           | 0.103       |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 457         |\n|    ep_rew_mean          | 5.28        |\n| time/                   |             |\n|    fps                  | 66          |\n|    iterations           | 6           |\n|    time_elapsed         | 743         |\n|    total_timesteps      | 49152       |\n| train/                  |             |\n|    approx_kl            | 0.041159015 |\n|    clip_fraction        | 0.36        |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.2        |\n|    explained_variance   | 0.616       |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.0689     |\n|    n_updates            | 50          |\n|    policy_gradient_loss | -0.0701     |\n|    value_loss           | 0.103       |\n-----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 472        |\n|    ep_rew_mean          | 5.54       |\n| time/                   |            |\n|    fps                  | 65         |\n|    iterations           | 7          |\n|    time_elapsed         | 881        |\n|    total_timesteps      | 57344      |\n| train/                  |            |\n|    approx_kl            | 0.05052234 |\n|    clip_fraction        | 0.381      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -1.16      |\n|    explained_variance   | 0.674      |\n|    learning_rate        | 0.0003     |\n|    loss                 | -0.0744    |\n|    n_updates            | 60         |\n|    policy_gradient_loss | -0.0748    |\n|    value_loss           | 0.0887     |\n----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 500         |\n|    ep_rew_mean          | 6.07        |\n| time/                   |             |\n|    fps                  | 64          |\n|    iterations           | 8           |\n|    time_elapsed         | 1022        |\n|    total_timesteps      | 65536       |\n| train/                  |             |\n|    approx_kl            | 0.059095338 |\n|    clip_fraction        | 0.408       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.12       |\n|    explained_variance   | 0.65        |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.0867     |\n|    n_updates            | 70          |\n|    policy_gradient_loss | -0.0742     |\n|    value_loss           | 0.0995      |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 530         |\n|    ep_rew_mean          | 6.68        |\n| time/                   |             |\n|    fps                  | 63          |\n|    iterations           | 9           |\n|    time_elapsed         | 1162        |\n|    total_timesteps      | 73728       |\n| train/                  |             |\n|    approx_kl            | 0.062031414 |\n|    clip_fraction        | 0.42        |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.1        |\n|    explained_variance   | 0.705       |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.109      |\n|    n_updates            | 80          |\n|    policy_gradient_loss | -0.0774     |\n|    value_loss           | 0.0809      |\n-----------------------------------------\n-----------------------------------------\n| rollout/                |             |\n|    ep_len_mean          | 519         |\n|    ep_rew_mean          | 6.51        |\n| time/                   |             |\n|    fps                  | 63          |\n|    iterations           | 10          |\n|    time_elapsed         | 1299        |\n|    total_timesteps      | 81920       |\n| train/                  |             |\n|    approx_kl            | 0.076943666 |\n|    clip_fraction        | 0.449       |\n|    clip_range           | 0.2         |\n|    entropy_loss         | -1.06       |\n|    explained_variance   | 0.766       |\n|    learning_rate        | 0.0003      |\n|    loss                 | -0.112      |\n|    n_updates            | 90          |\n|    policy_gradient_loss | -0.0797     |\n|    value_loss           | 0.0741      |\n-----------------------------------------\n---------------------------------------\n| rollout/                |           |\n|    ep_len_mean          | 533       |\n|    ep_rew_mean          | 6.71      |\n| time/                   |           |\n|    fps                  | 62        |\n|    iterations           | 11        |\n|    time_elapsed         | 1435      |\n|    total_timesteps      | 90112     |\n| train/                  |           |\n|    approx_kl            | 0.0861644 |\n|    clip_fraction        | 0.465     |\n|    clip_range           | 0.2       |\n|    entropy_loss         | -1.04     |\n|    explained_variance   | 0.77      |\n|    learning_rate        | 0.0003    |\n|    loss                 | -0.0819   |\n|    n_updates            | 100       |\n|    policy_gradient_loss | -0.0816   |\n|    value_loss           | 0.0689    |\n---------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 532        |\n|    ep_rew_mean          | 6.96       |\n| time/                   |            |\n|    fps                  | 62         |\n|    iterations           | 12         |\n|    time_elapsed         | 1566       |\n|    total_timesteps      | 98304      |\n| train/                  |            |\n|    approx_kl            | 0.09791782 |\n|    clip_fraction        | 0.48       |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.997     |\n|    explained_variance   | 0.732      |\n|    learning_rate        | 0.0003     |\n|    loss                 | -0.127     |\n|    n_updates            | 110        |\n|    policy_gradient_loss | -0.0828    |\n|    value_loss           | 0.0723     |\n----------------------------------------\n----------------------------------------\n| rollout/                |            |\n|    ep_len_mean          | 561        |\n|    ep_rew_mean          | 7.46       |\n| time/                   |            |\n|    fps                  | 62         |\n|    iterations           | 13         |\n|    time_elapsed         | 1697       |\n|    total_timesteps      | 106496     |\n| train/                  |            |\n|    approx_kl            | 0.10642853 |\n|    clip_fraction        | 0.479      |\n|    clip_range           | 0.2        |\n|    entropy_loss         | -0.953     |\n|    explained_variance   | 0.717      |\n|    learning_rate        | 0.0003     |\n|    loss                 | -0.113     |\n|    n_updates            | 120        |\n|    policy_gradient_loss | -0.0823    |\n|    value_loss           | 0.0735     |\n----------------------------------------\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "\n",
      "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n</pre>\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model.save(\"ppo_model\")\n",
    "\n",
    "loaded_model = PPO.load(\"ppo_model\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T19:28:48.959021Z",
     "iopub.execute_input": "2023-12-30T19:28:48.959424Z",
     "iopub.status.idle": "2023-12-30T19:28:49.181053Z",
     "shell.execute_reply.started": "2023-12-30T19:28:48.959387Z",
     "shell.execute_reply": "2023-12-30T19:28:49.180007Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "get_breakout_time(env, loaded_model)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T19:35:58.854303Z",
     "iopub.execute_input": "2023-12-30T19:35:58.855083Z",
     "iopub.status.idle": "2023-12-30T19:36:00.913919Z",
     "shell.execute_reply.started": "2023-12-30T19:35:58.855047Z",
     "shell.execute_reply": "2023-12-30T19:36:00.912877Z"
    },
    "trusted": true
   },
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "text": "Episode: 0 | Score: [5. 4. 4. 1.]\nEpisode: 1 | Score: [0. 0. 0. 0.]\nEpisode: 2 | Score: [0. 0. 0. 1.]\nEpisode: 3 | Score: [0. 0. 0. 0.]\nEpisode: 4 | Score: [0. 0. 0. 0.]\nEpisode: 5 | Score: [1. 0. 0. 0.]\nEpisode: 6 | Score: [0. 1. 0. 0.]\nEpisode: 7 | Score: [0. 0. 0. 0.]\nEpisode: 8 | Score: [0. 0. 0. 0.]\nEpisode: 9 | Score: [0. 0. 0. 0.]\n\nMean game time: 8.0 frames\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def get_game_frames(env, model):\n",
    "    frames = []\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = np.array([False] * 4)\n",
    "    while not done.any():\n",
    "        frames.append(env.render(mode='rgb_array'))\n",
    "        action, _ = model.predict(state)\n",
    "        state, _, done, _ = env.step(action)\n",
    "    \n",
    "    return frames"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T19:30:56.543479Z",
     "iopub.execute_input": "2023-12-30T19:30:56.543879Z",
     "iopub.status.idle": "2023-12-30T19:30:56.550757Z",
     "shell.execute_reply.started": "2023-12-30T19:30:56.543848Z",
     "shell.execute_reply": "2023-12-30T19:30:56.549586Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import imageio\n",
    "from IPython.display import Video\n",
    "\n",
    "imageio.mimsave(\"breakout_game.mp4\", get_game_frames(env, enhanced_model), format='FFMPEG')\n",
    "Video(\"breakout_game.mp4\", embed=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-30T19:33:11.944773Z",
     "iopub.execute_input": "2023-12-30T19:33:11.945240Z",
     "iopub.status.idle": "2023-12-30T19:33:12.507009Z",
     "shell.execute_reply.started": "2023-12-30T19:33:11.945207Z",
     "shell.execute_reply": "2023-12-30T19:33:12.505564Z"
    },
    "trusted": true
   },
   "execution_count": 45,
   "outputs": [
    {
     "execution_count": 45,
     "output_type": "execute_result",
     "data": {
      "text/plain": "<IPython.core.display.Video object>",
      "text/html": "<video controls  >\n <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\">\n Your browser does not support the video tag.\n </video>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Можно заметить, что время игры увеличилось"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
